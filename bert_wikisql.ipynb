{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"BJgD6w02Seoh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742527938305,"user_tz":420,"elapsed":147198,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}},"outputId":"90251315-66b4-4609-efa4-a0325afafb71"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n","sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"]},{"cell_type":"code","source":["!python -m bitsandbytes\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GQSPFrnXyxZx","executionInfo":{"status":"ok","timestamp":1742528533173,"user_tz":420,"elapsed":5910,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}},"outputId":"ed8695c2-066f-492d-a736-327c626343a1"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n","\n","===================================BUG REPORT===================================\n","/usr/local/lib/python3.11/dist-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: Welcome to bitsandbytes. For bug reports, please run\n","\n","python -m bitsandbytes\n","\n","\n","  warn(msg)\n","================================================================================\n","The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/python3.11/dist-packages/cv2/../../lib64')}\n","/usr/local/lib/python3.11/dist-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n","  warn(msg)\n","The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n","The following directories listed in your path were found to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('http'), PosixPath('8013')}\n","The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-203l0x1024s1k --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true --enable_kernel_event_logging=true ')}\n","The following directories listed in your path were found to be non-existent: {PosixPath('/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages')}\n","The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n","The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n","CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n","DEBUG: Possible options found for libcudart.so: {PosixPath('/usr/local/cuda/lib64/libcudart.so')}\n","CUDA SETUP: PyTorch settings found: CUDA_VERSION=124, Highest Compute Capability: 7.5.\n","CUDA SETUP: To manually override the PyTorch CUDA version please see:https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n","CUDA SETUP: Required library version not found: libbitsandbytes_cuda124.so. Maybe you need to compile it from source?\n","CUDA SETUP: Defaulting to libbitsandbytes_cpu.so...\n","\n","================================================ERROR=====================================\n","CUDA SETUP: CUDA detection failed! Possible reasons:\n","1. You need to manually override the PyTorch CUDA version. Please see: \"https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n","2. CUDA driver not installed\n","3. CUDA not installed\n","4. You have multiple conflicting CUDA libraries\n","5. Required library not pre-compiled for this bitsandbytes release!\n","CUDA SETUP: If you compiled from source, try again with `make CUDA_VERSION=DETECTED_CUDA_VERSION` for example, `make CUDA_VERSION=113`.\n","CUDA SETUP: The CUDA version for the compile might depend on your conda install. Inspect CUDA version via `conda list | grep cuda`.\n","================================================================================\n","\n","CUDA SETUP: Something unexpected happened. Please compile from source:\n","git clone https://github.com/TimDettmers/bitsandbytes.git\n","cd bitsandbytes\n","CUDA_VERSION=124\n","python setup.py install\n","CUDA SETUP: Setup Failed!\n","Traceback (most recent call last):\n","  File \"<frozen runpy>\", line 189, in _run_module_as_main\n","  File \"<frozen runpy>\", line 148, in _get_module_details\n","  File \"<frozen runpy>\", line 112, in _get_module_details\n","  File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/__init__.py\", line 6, in <module>\n","    from . import cuda_setup, utils, research\n","  File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/research/__init__.py\", line 1, in <module>\n","    from . import nn\n","  File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/research/nn/__init__.py\", line 1, in <module>\n","    from .modules import LinearFP8Mixed, LinearFP8Global\n","  File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/research/nn/modules.py\", line 8, in <module>\n","    from bitsandbytes.optim import GlobalOptimManager\n","  File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/optim/__init__.py\", line 6, in <module>\n","    from bitsandbytes.cextension import COMPILED_WITH_CUDA\n","  File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/cextension.py\", line 20, in <module>\n","    raise RuntimeError('''\n","RuntimeError: \n","        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n","\n","        python -m bitsandbytes\n","\n","        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n","        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n","        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"zsjUeaAITJC9","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1742527970539,"user_tz":420,"elapsed":32231,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}},"outputId":"9b348e71-4615-465d-e982-78c94f031c58"},"outputs":[{"output_type":"stream","name":"stdout","text":["False\n","\n","===================================BUG REPORT===================================\n","================================================================================\n","The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/python3.11/dist-packages/cv2/../../lib64')}\n","The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n","The following directories listed in your path were found to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('http'), PosixPath('8013')}\n","The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-203l0x1024s1k --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true --enable_kernel_event_logging=true '), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n","The following directories listed in your path were found to be non-existent: {PosixPath('/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages')}\n","The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n","The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n","CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n","DEBUG: Possible options found for libcudart.so: {PosixPath('/usr/local/cuda/lib64/libcudart.so')}\n","CUDA SETUP: PyTorch settings found: CUDA_VERSION=124, Highest Compute Capability: 7.5.\n","CUDA SETUP: To manually override the PyTorch CUDA version please see:https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n","CUDA SETUP: Required library version not found: libbitsandbytes_cuda124.so. Maybe you need to compile it from source?\n","CUDA SETUP: Defaulting to libbitsandbytes_cpu.so...\n","\n","================================================ERROR=====================================\n","CUDA SETUP: CUDA detection failed! Possible reasons:\n","1. You need to manually override the PyTorch CUDA version. Please see: \"https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n","2. CUDA driver not installed\n","3. CUDA not installed\n","4. You have multiple conflicting CUDA libraries\n","5. Required library not pre-compiled for this bitsandbytes release!\n","CUDA SETUP: If you compiled from source, try again with `make CUDA_VERSION=DETECTED_CUDA_VERSION` for example, `make CUDA_VERSION=113`.\n","CUDA SETUP: The CUDA version for the compile might depend on your conda install. Inspect CUDA version via `conda list | grep cuda`.\n","================================================================================\n","\n","CUDA SETUP: Something unexpected happened. Please compile from source:\n","git clone https://github.com/TimDettmers/bitsandbytes.git\n","cd bitsandbytes\n","CUDA_VERSION=124\n","python setup.py install\n","CUDA SETUP: Setup Failed!\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: Welcome to bitsandbytes. For bug reports, please run\n","\n","python -m bitsandbytes\n","\n","\n","  warn(msg)\n","/usr/local/lib/python3.11/dist-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: /usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n","  warn(msg)\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Failed to import transformers.trainer_seq2seq because of the following error (look up to see its traceback):\n\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_seq2seq.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# isort: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m from .integrations import (\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mget_reporting_integration_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrainer_callback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProgressCallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainerCallback\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPREFIX_CHECKPOINT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBestRun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntervalStrategy\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIntervalStrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtraining_args\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAcceleratorState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPartialState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistributedType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccelerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m from .big_modeling import (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcheckpointing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_accelerator_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_custom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_accelerator_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_custom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoaderDispatcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepare_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_first_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/checkpointing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m from .utils import (\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbnb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhas_4bit_bnb_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_and_quantize_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfsdp_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_fsdp_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_fsdp_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_fsdp_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_fsdp_optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/bnb.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_bnb_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mbitsandbytes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcuda_setup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m from .autograd._functions import (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/research/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from .autograd._functions import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mswitchback_bnb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/research/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearFP8Mixed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearFP8Global\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/research/nn/modules.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitsandbytes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbitsandbytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlobalOptimManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbitsandbytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOutlierTracer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfind_outlier_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/optim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbitsandbytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOMPILED_WITH_CUDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/cextension.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mCUDASetup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_log_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         raise RuntimeError('''\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mCUDA\u001b[0m \u001b[0mSetup\u001b[0m \u001b[0mfailed\u001b[0m \u001b[0mdespite\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0mbeing\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mrun\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfollowing\u001b[0m \u001b[0mcommand\u001b[0m \u001b[0mto\u001b[0m \u001b[0mget\u001b[0m \u001b[0mmore\u001b[0m \u001b[0minformation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: \n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-428adff573db>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataCollatorForSeq2Seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeq2SeqTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeq2SeqTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1102\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.trainer_seq2seq because of the following error (look up to see its traceback):\n\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues"]}],"source":["import pandas as pd\n","from datasets import load_dataset\n","from transformers import AutoTokenizer\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","from transformers import DataCollatorForSeq2Seq\n","#from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","\n","\n","# ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"rG7jI-w_TWLz"},"source":["# Data Processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zh72lqGDTPyw","executionInfo":{"status":"aborted","timestamp":1742527970521,"user_tz":420,"elapsed":179532,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["dataset = load_dataset(\"wikisql\", trust_remote_code=True)\n","dataset"]},{"cell_type":"markdown","metadata":{"id":"e_vquoXJTiqZ"},"source":["### Tokenisation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SAJhmiP1Tb-N","executionInfo":{"status":"aborted","timestamp":1742527970536,"user_tz":420,"elapsed":179546,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["model_name =  \"facebook/bart-base\"\n","# instantiate tokenizer of llama2\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qQkg9BSbTsVa","executionInfo":{"status":"aborted","timestamp":1742527970537,"user_tz":420,"elapsed":179546,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["# test the functionality of the tokenizer\n","\n","question = dataset['train'][0]['question']\n","print('Question:', question)\n","\n","tokenized_example = tokenizer(dataset['train'][0]['question'])\n","print('After tokenisation:', tokenized_example)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I5g6XhK1T1e4","executionInfo":{"status":"aborted","timestamp":1742527970537,"user_tz":420,"elapsed":179546,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["# finding the max length for tokenization\n","tokenizer.model_max_length"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"whVSms_pT3Gp","executionInfo":{"status":"aborted","timestamp":1742527970538,"user_tz":420,"elapsed":179546,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","def preprocess_function(examples):\n","    inputs = []\n","    for question, table in zip(examples['question'], examples['table']):\n","        table_str = \" | \".join([f\"{header} ({type})\" for header, type in zip(table['header'], table['types'])])\n","        input_str = f\"Convert the Question to SQL: {question}, based on the table: {table_str}\"\n","        inputs.append(input_str)\n","\n","    targets = [sql['human_readable'] for sql in examples['sql']]\n","\n","    ### tokenise the input (prompt) and answers (sql queries)\n","    model_inputs = tokenizer(inputs, padding='max_length', max_length=512, truncation=True, return_tensors='pt')\n","    labels = tokenizer(targets, padding='max_length', max_length=128, truncation=True, return_tensors='pt')\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yMEcR5-kgyW7","executionInfo":{"status":"aborted","timestamp":1742527970538,"user_tz":420,"elapsed":179546,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["train_set = dataset['train'].map(preprocess_function, batched=True, remove_columns=dataset['train'].column_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XUDbxHJbgvBO","executionInfo":{"status":"aborted","timestamp":1742527970538,"user_tz":420,"elapsed":179545,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["validation_set = dataset['validation'].map(preprocess_function, batched=True, remove_columns=dataset['validation'].column_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cJ_f_Ezog0QK","executionInfo":{"status":"aborted","timestamp":1742527970538,"user_tz":420,"elapsed":179544,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["test_set = dataset['test'].map(preprocess_function, batched=True, remove_columns=dataset['test'].column_names)"]},{"cell_type":"markdown","metadata":{"id":"QWm9955LU6M8"},"source":["# Model Training"]},{"cell_type":"markdown","metadata":{"id":"Hq7zKISvU2xu"},"source":["### Evaluation Metric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mWm_VoXVUY5L","executionInfo":{"status":"aborted","timestamp":1742527970539,"user_tz":420,"elapsed":179544,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["!pip install evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JzRVp1rXU_Xn","executionInfo":{"status":"aborted","timestamp":1742527970561,"user_tz":420,"elapsed":179565,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["from evaluate import load\n","bleu_metric = load(\"bleu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3qyPdxxVC89","executionInfo":{"status":"aborted","timestamp":1742527970562,"user_tz":420,"elapsed":179566,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["def compute_metrics(pred):\n","    # Retrive predicted tokensw\n","    labels_ids = pred.label_ids\n","    pred_ids = pred.predictions\n","\n","    # Decode the tokens and convert to text\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    labels_ids[labels_ids == -100] = tokenizer.pad_token_id # n the labels_ids array, the value -100 represents ignored tokens (often used during loss computation to ignore padding). It is replaced with the padding token ID (tokenizer.pad_token_id) to avoid errors during decoding.\n","    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n","\n","    # Compute BLEU score\n","    '''\n","    BLEU Metric:\n","\n","    Measures how closely predicted text matches the reference text.\n","    Compares n-grams in predictions with those in references.\n","\n","    Input Format:\n","\n","    predictions: A list of tokenized predicted strings (pred_str split into words).\n","    references: A list of lists of tokenized reference strings (label_str split into words).\n","    Each reference is nested in a list to support multiple references per prediction.\n","    '''\n","\n","    bleu = bleu_metric.compute(predictions=pred_str, references=label_str)\n","    # bleu = bleu_metric.compute(predictions=[p.split() for p in pred_str], references=[[l.split()] for l in label_str])\n","\n","    return {\"bleu\": bleu[\"bleu\"]}"]},{"cell_type":"markdown","metadata":{"id":"lFEB8NvGVggG"},"source":["### Model Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PZ7bz-9lbzSd","executionInfo":{"status":"aborted","timestamp":1742527970562,"user_tz":420,"elapsed":179566,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["# DataCollatorForSeq2Seq dynamically pads the sentences to the longest length in a batch during\n","# collation, instead of padding the whole dataset to the maximum length.\n","# These elements are of the same type as the elements of train_dataset or eval_dataset.\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_name, padding=True, return_tensors='pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5rHR-t2Vlrk","executionInfo":{"status":"aborted","timestamp":1742527970562,"user_tz":420,"elapsed":179566,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","model_name = \"facebook/bart-base\"\n","# instantiate tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"./bart_baseline_results1\",\n","    per_device_eval_batch_size=16,\n","    predict_with_generate=True,\n","    fp16=True,\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_set,\n","    eval_dataset=test_set,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"markdown","metadata":{"id":"T_ohqI0acpl6"},"source":["### Results on Baseline Model (Before Fine Tuning)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j2wWCI-HcSRv","executionInfo":{"status":"aborted","timestamp":1742527970563,"user_tz":420,"elapsed":179566,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3evPg8gccwGI","executionInfo":{"status":"aborted","timestamp":1742527970563,"user_tz":420,"elapsed":179565,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["import pandas as pd\n","\n","# Select some samples\n","sample_range = range(20)\n","test_examples = test_set.select(sample_range)\n","\n","# Make predictions\n","pred = trainer.predict(test_examples)\n","# Retrive predicted tokens\n","pred_ids = pred.predictions\n","\n","# Decode the prediction\n","pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","\n","# Create a DataFrame\n","questions = [example['question'] for example in dataset['test'].select(sample_range)]\n","header = [example['table']['header'] for example in dataset['test'].select(sample_range)]\n","labels = [example['sql']['human_readable'] for example in dataset['test'].select(sample_range)]\n","\n","df = pd.DataFrame({\n","    \"Question\": questions,\n","    \"Header\": header,\n","    \"Prediction\": pred_str,\n","    \"Target\": labels\n","})\n","\n","# Show all columns & rows\n","pd.set_option('display.max_colwidth', None)\n","df.head(10)"]},{"cell_type":"markdown","metadata":{"id":"kThKhhRaeKj1"},"source":["### Model Fine Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CHB-RoIYeI7s","executionInfo":{"status":"aborted","timestamp":1742527970564,"user_tz":420,"elapsed":179566,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n","\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","\n","peft_config = LoraConfig(\n","    task_type=TaskType.SEQ_2_SEQ_LM,\n","    inference_mode=False,\n","    r=8,\n","    lora_alpha=32,\n","    lora_dropout=0.1,\n",")\n","\n","model = get_peft_model(model, peft_config)\n","model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uIH-F2ereXum","executionInfo":{"status":"aborted","timestamp":1742527970565,"user_tz":420,"elapsed":179567,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["# hyperparameters in the training_args are cited from https://github.com/anyuanay/medium/blob/main/src/working_huggingface/Working_with_HuggingFace_ch3_Fine_Tuning_T5_Small_Text_Summarization_Model.ipynb\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"sanmit_bart_wikisql\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=1e-4,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    weight_decay=0.01,\n","    save_total_limit=3,\n","    num_train_epochs=10,\n","    predict_with_generate=True,\n","    fp16=True,\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_set,\n","    eval_dataset=validation_set,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ihar65Mmem8f","executionInfo":{"status":"aborted","timestamp":1742527970565,"user_tz":420,"elapsed":179566,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqwnA5DyetyJ","executionInfo":{"status":"aborted","timestamp":1742527970565,"user_tz":420,"elapsed":179566,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["# # mount gdrive\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# # Save the model\n","# model_folder_path = \"/content/drive/MyDrive/sanmit_bart_finetuned1_wikisql\"\n","\n","# model.save_pretrained(model_folder_path)\n","# tokenizer.save_pretrained(model_folder_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jXWDmRNyFvmf","executionInfo":{"status":"aborted","timestamp":1742527970566,"user_tz":420,"elapsed":179566,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["hf_model_name = \"sanmit08/bart_finetuned10epochs_wikisql\"\n","model.push_to_hub(hf_model_name, use_auth_token=\"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\")\n","tokenizer.push_to_hub(hf_model_name, use_auth_token=\"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\")"]},{"cell_type":"markdown","metadata":{"id":"fxqgtqEee05Q"},"source":["# Evaluating after Fine Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EfpbyjGXe3xm","executionInfo":{"status":"aborted","timestamp":1742527970566,"user_tz":420,"elapsed":179566,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["# load fine-tuned model\n","model_folder_path = hf_model_name\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_folder_path)\n","tokenizer = AutoTokenizer.from_pretrained(model_folder_path)\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"./results\",\n","    per_device_eval_batch_size=16,\n","    predict_with_generate=True,\n","    fp16=True,\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_set,\n","    eval_dataset=test_set,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dI69pWGUfO0K","executionInfo":{"status":"aborted","timestamp":1742527970574,"user_tz":420,"elapsed":179573,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P2Vp6V11fQdA","executionInfo":{"status":"aborted","timestamp":1742527970575,"user_tz":420,"elapsed":179574,"user":{"displayName":"Nirman Malaviya","userId":"11774304362402845202"}}},"outputs":[],"source":["test_examples = test_set.select(sample_range)\n","\n","# Make predictions\n","pred = trainer.predict(test_examples)\n","# Retrive predicted tokens\n","pred_ids = pred.predictions\n","\n","# Decode the prediction\n","pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","\n","# Create a DataFrame\n","questions = [example['question'] for example in dataset['test'].select(sample_range)]\n","header = [example['table']['header'] for example in dataset['test'].select(sample_range)]\n","labels = [example['sql']['human_readable'] for example in dataset['test'].select(sample_range)]\n","\n","df = pd.DataFrame({\n","    \"Question\": questions,\n","    \"Header\": header,\n","    \"Prediction\": pred_str,\n","    \"Target\": labels\n","})\n","\n","# Show all columns & rows\n","pd.set_option('display.max_colwidth', None)\n","df.head(10)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}