{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1dc004c8eb7c4303a93bede51492c8e4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8f82fcf0f6144e9fa86b0ddfdd93dedc","IPY_MODEL_7e8b69dc157e44d3bb862de77d980dda","IPY_MODEL_bb5d95dc91024d4297f7f833316a6cc2"],"layout":"IPY_MODEL_a457c20c1ee241589be0e548c687d605"}},"8f82fcf0f6144e9fa86b0ddfdd93dedc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5983cd233fb24dc0b4a8351ce5d589f9","placeholder":"​","style":"IPY_MODEL_660e34576ca94cd3b0389956ef38bf5a","value":"README.md: 100%"}},"7e8b69dc157e44d3bb862de77d980dda":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f2e88ac65174cd0b708ff4b283de8b3","max":7797,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1233415b91234086a176c0a037656444","value":7797}},"bb5d95dc91024d4297f7f833316a6cc2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48457feb46494f069ab4cd17ad042231","placeholder":"​","style":"IPY_MODEL_3c6384809b4f45469ead31a1acaa0573","value":" 7.80k/7.80k [00:00&lt;00:00, 838kB/s]"}},"a457c20c1ee241589be0e548c687d605":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5983cd233fb24dc0b4a8351ce5d589f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"660e34576ca94cd3b0389956ef38bf5a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f2e88ac65174cd0b708ff4b283de8b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1233415b91234086a176c0a037656444":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"48457feb46494f069ab4cd17ad042231":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c6384809b4f45469ead31a1acaa0573":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45a983c8addc4009ab2b1469ebcb8b38":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0a622781499b4e8e8385ca3f4f7d7623","IPY_MODEL_4e4185be2f7a4af2833930a3b54c0f94","IPY_MODEL_6f64a19251ec4cd88047f5ddbfa24b09"],"layout":"IPY_MODEL_33eff2944f204622b1a4d2808328e169"}},"0a622781499b4e8e8385ca3f4f7d7623":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_277a644d5bf64354a3c819559277e458","placeholder":"​","style":"IPY_MODEL_85c2c9d54be343e286d15acd5a605598","value":"wikisql.py: 100%"}},"4e4185be2f7a4af2833930a3b54c0f94":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7de4050600c4f08977c8d8574d72d5c","max":6573,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0daaab47fc0141adb46e9b6dbc66ffb4","value":6573}},"6f64a19251ec4cd88047f5ddbfa24b09":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6614cbead1047af8b9b26990b1425c2","placeholder":"​","style":"IPY_MODEL_e10027d611e8425281bd27c1592549ce","value":" 6.57k/6.57k [00:00&lt;00:00, 714kB/s]"}},"33eff2944f204622b1a4d2808328e169":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"277a644d5bf64354a3c819559277e458":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85c2c9d54be343e286d15acd5a605598":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7de4050600c4f08977c8d8574d72d5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0daaab47fc0141adb46e9b6dbc66ffb4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f6614cbead1047af8b9b26990b1425c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e10027d611e8425281bd27c1592549ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# Install necessary libraries\n","!pip install transformers datasets evaluate\n","\n","# Import required modules\n","from datasets import load_dataset\n","from transformers import BartForConditionalGeneration, BartTokenizer, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n","import evaluate\n","import numpy as np\n","\n","dataset = load_dataset(\"wikisql\")\n","print(dataset[\"train\"][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1dc004c8eb7c4303a93bede51492c8e4","8f82fcf0f6144e9fa86b0ddfdd93dedc","7e8b69dc157e44d3bb862de77d980dda","bb5d95dc91024d4297f7f833316a6cc2","a457c20c1ee241589be0e548c687d605","5983cd233fb24dc0b4a8351ce5d589f9","660e34576ca94cd3b0389956ef38bf5a","3f2e88ac65174cd0b708ff4b283de8b3","1233415b91234086a176c0a037656444","48457feb46494f069ab4cd17ad042231","3c6384809b4f45469ead31a1acaa0573","45a983c8addc4009ab2b1469ebcb8b38","0a622781499b4e8e8385ca3f4f7d7623","4e4185be2f7a4af2833930a3b54c0f94","6f64a19251ec4cd88047f5ddbfa24b09","33eff2944f204622b1a4d2808328e169","277a644d5bf64354a3c819559277e458","85c2c9d54be343e286d15acd5a605598","d7de4050600c4f08977c8d8574d72d5c","0daaab47fc0141adb46e9b6dbc66ffb4","f6614cbead1047af8b9b26990b1425c2","e10027d611e8425281bd27c1592549ce"]},"id":"nPecWjR1_lH2","outputId":"2a80a0b1-7a3c-497f-ccbf-550662946f83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n","Collecting datasets\n","  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n","Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.0\n","    Uninstalling fsspec-2025.3.0:\n","      Successfully uninstalled fsspec-2025.3.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.4.1 dill-0.3.8 evaluate-0.4.3 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/7.80k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dc004c8eb7c4303a93bede51492c8e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["wikisql.py:   0%|          | 0.00/6.57k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45a983c8addc4009ab2b1469ebcb8b38"}},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VdTQXkvFDD5q"},"outputs":[],"source":["\n","\n","# -----------------------------------------------------------\n","# Step 1: Environment Setup\n","# -----------------------------------------------------------\n","# We install and import all libraries needed to load the dataset,\n","# initialize a pre-trained BART model, tokenize our data, and evaluate\n","# our results using BLEU and ROUGE metrics.\n","\n","# -----------------------------------------------------------\n","# Step 2: Data Preparation\n","# -----------------------------------------------------------\n","# Load the SQL generation dataset from Hugging Face.\n","# The dataset \"b-mc2/sql-create-context\" contains samples with 'context', 'question', and 'answer'.\n","\n","\n","# Split the training data into a training and validation set (80/20 split).\n","split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n","train_dataset = split_dataset[\"train\"]\n","val_dataset = split_dataset[\"test\"]\n","\n","# -----------------------------------------------------------\n","# Step 3: Preprocessing & Tokenization\n","# -----------------------------------------------------------\n","# Load the pre-trained BART tokenizer and model.\n","model_name = \"facebook/bart-base\"\n","tokenizer = BartTokenizer.from_pretrained(model_name)\n","model = BartForConditionalGeneration.from_pretrained(model_name)\n","\n","# Define a preprocessing function that:\n","# 1. Combines the 'context' and 'question' fields to form the input.\n","# 2. Uses the 'answer' field as the target (SQL query).\n","# 3. Tokenizes both the inputs and targets.\n","def preprocess_function(examples):\n","    # Use question as input; optionally, add table header\n","    inputs = [q + \" | \" + \" , \".join(t[\"header\"]) for q, t in zip(examples[\"question\"], examples[\"table\"])]\n","\n","    # Use the human-readable SQL as target\n","    targets = [sql[\"human_readable\"] for sql in examples[\"sql\"]]\n","\n","    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n","\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","\n","\n","# Apply the preprocessing function to the train and validation datasets.\n","# batched=True ensures that the function processes multiple samples at once.\n","train_dataset = train_dataset.map(preprocess_function, batched=True)\n","val_dataset = val_dataset.map(preprocess_function, batched=True)\n","\n","# -----------------------------------------------------------\n","# Step 4: Data Collator\n","# -----------------------------------------------------------\n","# The DataCollatorForSeq2Seq automatically pads the inputs and labels to the maximum length in the batch.\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","\n"]},{"cell_type":"code","source":["# !pip install --upgrade transformers\n"],"metadata":{"id":"VUDOQw65E5zS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"g-Z0lMFTFGy6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----------------------------------------------------------\n","# Step 5: Fine-Tuning Setup\n","# -----------------------------------------------------------\n","# Define the training arguments for the Hugging Face Trainer.\n","# These include output directory, learning rate, batch size, number of epochs, etc.\n","#from transformers import BartForConditionalGeneration, BartTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",              # check pointing each epoch\n","    save_total_limit=2,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    predict_with_generate=True,\n","    fp16=True,\n",")\n"],"metadata":{"id":"pvDl0nSADQ5W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install rouge_score\n"],"metadata":{"id":"erl88OJHFIpi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# -----------------------------------------------------------\n","# Step 6: Evaluation Metrics Setup (BLEU and ROUGE)\n","# -----------------------------------------------------------\n","# Define a compute_metrics function to evaluate model predictions using BLEU and ROUGE.\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","\n","    # Replace -100 (the default ignore index for labels) with the tokenizer's pad token id.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","\n","    # Decode the predictions and labels into human-readable text.\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Load the BLEU and ROUGE evaluation metrics.\n","    bleu_metric = evaluate.load(\"bleu\")\n","    rouge_metric = evaluate.load(\"rouge\")\n","\n","    # Compute BLEU; note that the reference for BLEU should be a list of lists.\n","    bleu = bleu_metric.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n","\n","    # Compute ROUGE scores.\n","    rouge = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n","\n","    # Extract the BLEU score and ROUGE F1 scores for ROUGE-1, ROUGE-2, and ROUGE-L.\n","    return {\n","        \"bleu\": bleu[\"bleu\"],\n","        \"rouge1\": rouge[\"rouge1\"],\n","        \"rouge2\": rouge[\"rouge2\"],\n","        \"rougeL\": rouge[\"rougeL\"],\n","    }\n"],"metadata":{"id":"t2s1ZmNFEYEC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from transformers.trainer_utils import get_last_checkpoint\n"],"metadata":{"id":"8qPBVsnKQcMs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a safe conversion function that filters out None tokens\n","def safe_convert_tokens_to_string(tokens):\n","    return \"\".join([token if token is not None else \"\" for token in tokens])\n","\n","# Override the tokenizer's convert_tokens_to_string method with our safe version\n","tokenizer.convert_tokens_to_string = safe_convert_tokens_to_string\n","\n","# Update compute_metrics to define computed_metric_value\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    # Decode predictions and labels\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    # Compute your desired metric here.\n","    # For demonstration, we're using a dummy value. Replace this with your metric computation (e.g., BLEU, ROUGE, etc.)\n","    computed_metric_value = 0.0\n","    return {\"metric_name\": computed_metric_value}"],"metadata":{"id":"JePqZ-kNobZg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# -----------------------------------------------------------\n","# Step 7: Trainer Initialization and Model Fine-Tuning\n","# -----------------------------------------------------------\n","# Initialize the Trainer with our model, training arguments, datasets, data collator, and evaluation metrics.\n","\n","# Initialize the Trainer with our model, training arguments, datasets, data collator, and evaluation metrics.\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Fine-tune the pre-trained BART model on our SQL generation task.\n","last_checkpoint = None\n","if os.path.isdir(training_args.output_dir):\n","    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","\n","if last_checkpoint is not None:\n","    print(f\"Resuming from checkpoint: {last_checkpoint}\")\n","    trainer.train(resume_from_checkpoint=last_checkpoint)\n","else:\n","    print(\"No checkpoint found. Starting training from scratch.\")\n","    trainer.train()\n","\n","    # Nirman Key: 3c756d61f1e64f4a7716d57a61805f7158a99f3d"],"metadata":{"id":"wQGXvpw2EsfE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----------------------------------------------------------\n","# Step 8: Model Evaluation\n","# -----------------------------------------------------------\n","# Save the final trained model (in addition to the checkpoints).\n","trainer.save_model(\"./results\")  # or any path you prefer\n","\n","# Evaluate the model on the validation set and print the BLEU and ROUGE scores.\n","results = trainer.evaluate()\n","print(\"Evaluation Results:\")\n","print(results)\n"],"metadata":{"id":"_l5D7gZdEvKC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----------------------------------------------------------\n","# Step 9: Inference and Generation\n","# -----------------------------------------------------------\n","# Generate predictions on a small sample from the validation set to visually inspect the outputs.\n","# Here we select 5 samples from the validation set.\n","sample_dataset = val_dataset.select(range(5))\n","predictions = trainer.predict(sample_dataset)\n","\n","# Decode the model predictions and the corresponding ground truth labels.\n","decoded_preds = tokenizer.batch_decode(predictions.predictions, skip_special_tokens=True)\n","decoded_labels = tokenizer.batch_decode(predictions.label_ids, skip_special_tokens=True)\n","\n","# Display the results for comparison.\n","for i, (pred, label) in enumerate(zip(decoded_preds, decoded_labels)):\n","    print(f\"\\nSample {i + 1}:\")\n","    print(\"Generated SQL Query:\", pred)\n","    print(\"Ground Truth SQL Query:\", label)"],"metadata":{"id":"4uPY_RouLNLc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8pVpIuLcioJ6"},"execution_count":null,"outputs":[]}]}